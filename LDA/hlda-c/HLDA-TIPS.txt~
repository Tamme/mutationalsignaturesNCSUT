I see the following patterns in my run of HLDA on mallet with 6 levels,
alpha = 10.0, gamma = 1.0 and eta = 0.1 ( other than level these are
default ).

http://www.cs.princeton.edu/~blei/papers/BleiGriffithsJordan2009.pdf

  Check if you get reasonable number of topics. If not, you should
decrease gamma and increase eta. In the initialization step, each document
added into tree as a new path then levels are sampled and the path is
sampled again. This creates many topics to start with.

DEPTH values from 4 to 7
>ETA 1.0 0.5 0.25 0.125 for direichlet (theta i think)
>GAM 0.5 0.5 0.5 gamma 
>GEM_MEAN 0.35
>GEM_SCALE 100
>SCALING_SHAPE 1.0
>SCALING_SCALE 0.5
>SAMPLE_ETA 0
>SAMPLE_GEM 0
1. maybe the GAMMA hyper-parameter is too small, which will limit the possible to generate a new branch.
2. and also you should check whether the iterator is convergent by verifying that all the result files is exists(such as mode.levels), especially when you select for non sampling.

When you specifiy the depth of the tree, you are limiting the number of 
topics a single document can exhibit. Professor Blei has two papers 
(that I know of). One is "Nested Chinese Restaurant Process and Bayesian 
non-parametric inference of topic hierarchies". In this paper, he has 
described the use of Nested Chinese Restaurant Process metaphor to 
describe an infinitely deep infinitely branching topic tree. (although 
the code on his website only implements fixed length trees). The idea of 
non-parametric bayesian statistics for topic hierarchies is that you do 
not assume the depth or branching factor of the tree but rather learn it 
from data (although you can exhibit a great degree of control on the 
structure of trees by specific values of hyper-parameters). I would 
recommend you to consider the paper "Hierarchical Dirichlet Process" by 
Yee Whye Teh as well to gain more insight into how a non-parametric 
model could be implemented from an algorithmic standpoint in the 
scenario of topic models.

HMMMM

SCORE -1.43335871824476e+005
ITER 104
ETA        1.91344580793593e-001 1.73419242997856e-001 1.21457502341850e-001
GAMMA      1.00000000000000e+000 1.00000000000000e+000
GEM_MEAN 9.47457147985311e-001
GEM_SCALE 6.79639697976705e+001
SCALING_SHAPE 5.00000000000000e-001
SCALING_SCALE 1.00000000000000e+000





1) A model with K topics, and N documents, and each document has only
one topic.  To generate a document, generate a topic for the document
then generate words from this topic.

2) LDA: Similar to above except that each *word* in a document is
generated from a separate topic.  To generate a document, generate
mixing weights over a fixed number of topics K, then generate each
word as a mixture of the document's topics (i.e. pick a topic
according to the mixing weights then use the topic to generate a
word).

3) A hierarchical LDA, but with a fixed tree.  It is assumed that each
tree has uniform depth L, although I don't see why this is necessary.
To generate a document, first pick a path from root to some leaf.  By
assumption this path is of length L in its nodes, and each node has a
topic associated with it.  Pick mixing weights of length L for the
topics of the nodes in the path.  Then generate each word as a mixture
of those L topics.

4) A generalized hierarchical LDA with no fixed tree structure.  Again
the depth is L, and in this case I *really* don't see the point of
this.  But anyway, instead of a fixed number of children per node, use
a CRP at each node controlling an indefinite number of children.
Again to generate a document we first pick a path from root to leaf.
But to do this, when we get to a node, we don't just randomly pick
among the children.  Instead, we consult the CRP, and it picks the
appropriate child and returns the associated topic -- periodically
creating new children/topics on the fly.  Again, once you've got the
path of length L, pick the mixing weights just as above and generate
words as a mixture of L topics, again like above.

You should rephrase your questions in terms of the above description
and in terms of which model you're referring to.

!!!!!!!!!!SEE HOW THE PARAMETERS CHANGE
I'm trying to use the hlda-c code on Prof. Blei's page. But there are some
strange experimental results. Is anybody here using the implementation and
facing the similar problems?

1. If using the MH_update method to update the hypermeter eta, no matter
which value was set initially, it would be changed to the same value around
0.5 at the highest-level and around 1.5 at other levels after a few
iterations.
The same thing happened on updating the GEM_MEAN, which was changed to
around 0.27.




INCREASE ITERATIONS



> DEPTH 4
> // The number of levels in the tree being learned

yes.

> ETA 1.0 0.5 0.25 0.125
> // I am not entirely sure, but I suspect this to be a prior on the distribution of words on a path through the tree. In this instance, for example, more words would be expected to be assigned to the root than to all the other nodes in the tree combined.

that's right---these are the parameters to the distribution on topics
at each level of the tree.


> GAM 0.5 0.5 0.5
> // This is the gamma used in the CRP; one value of gamma for each level in the tree. As I understand it, a small gamma for a level encourages fewer new branches at each node, while a large gamma encourages many branches (roughly speaking).
Gamma is Beta(1, GAM)
Beta distr online - 
http://keisan.casio.com/exec/system/1180573226

that's right.

> GEM_MEAN 0.35
> GEM_SCALE 100

these refer to the GEM distribution on topic proportions.  see this paper:

   http://www.cs.princeton.edu/~blei/papers/BleiGriffithsJordan2009a.pdf

this is used in the generative process (on page 7:11) in step 2b.  we
review the GEM distribution in section 2.3.

The GEM parameter m reflects the proportion of general words relative to specific
words, and the GEM parameter π reflects how strictly we expect the documents to
adhere to these proportions. A larger value of π enforces the notions of generality
and specificity that lead to more interpretable trees.
http://www.cs.princeton.edu/~blei/papers/BleiGriffithsJordan2009a.pdf


> SCALING_SHAPE 1.0
> SCALING_SCALE 0.5
> SAMPLE_ETA 0
> SAMPLE_GEM 0
> // These final 6 parameters I am more unsure about. By empirically adjusting the SCALING_* values, my tree seemed to become 'noisier' and 'cleaner', but it was hard to determine the exact effect they were having.

these control whether we sample the hyperparameters described above:
the GEM parameters and the dirichlet hyperparameter for the topics.
with SAMPLE_ETA and SAMPLE_GEM both set to 0, hyperparameter sampling
is turned off.  it makes sense that it was hard to determine this
effect :-)







Hi Diego,

If your document cluster does not split in lower levels you should check
two things. First how is CRP prior working in current node. Check If it
successfully assigns probability proportional to
SCALING_SCALE*SCALING_SHAPE (*t->scaling*) to a new topic. Second how is
your likelihood for empty topics. Because proposed path has hypothetical
empty topics after current node to fill the path up to tr->depth.

Best,
Halid



Dear all,

I am trying to use David Blei's implementation of hlda

(http://www.cs.princeton.edu/~blei/downloads/hlda-c.tgz). However, the

tests I have run so far (using about 2000 academic articles) have all

generated trees where all internal nodes have exactly one child - like

an extended star. I am wondering whether I am doing something wrong or

whether this can be normally expected from the algorithm. Could anyone

help me to understand the possible cause of this behavior?

The parameters that I am using are:

DEPTH values from 4 to 7

ETA 1.0 0.5 0.25 0.125

GAM 0.5 0.5 0.5

GEM_MEAN 0.35

GEM_SCALE 100

SCALING_SHAPE 1.0

SCALING_SCALE 0.5

SAMPLE_ETA 0

SAMPLE_GEM 0


./hlda gibbs ../data/lda_full_primarysite_new.txt settings_test5.txt 20160127_2/

python tree_distr.py txt 20160127_2/run004/mode ../data/gen_vocab.txt ../data/gen_vocab.txt hlda_full_primarysite_004

